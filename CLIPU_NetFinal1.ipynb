{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "vB_tYqlwhvAZ",
        "outputId": "c2219fff-c7ae-4622-dbab-64ea6361b0b0"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "#!pip install -q gdown\n",
        "\n",
        "!kaggle datasets download -d debeshjha1/kvasircapsuleseg\n",
        "\n",
        "!unzip -q kvasircapsuleseg.zip -d kvasir_raw\n",
        "\n",
        "!mkdir -p polyp_data/images polyp_data/masks\n",
        "!cp -r kvasir_raw/Kvasir-Capsule/images/* polyp_data/images/\n",
        "!cp -r kvasir_raw/Kvasir-Capsule/masks/* polyp_data/masks/\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"Images:\", len(os.listdir(\"polyp_data/images\")))\n",
        "print(\"Masks:\", len(os.listdir(\"polyp_data/masks\")))\n",
        "\n",
        "#!pip install -q transformers==4.40.0 pillow tqdm matplotlib scikit-image kaggle\n",
        "\n",
        "import os, random\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import trange, tqdm\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device:', device)\n",
        "\n",
        "#!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "#!pip install -q albumentations==1.4.6\n",
        "\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import trange, tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from einops import rearrange\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "IMG_SIZE = 256\n",
        "CLIP_IMG_SIZE = 224\n",
        "SUPPORT_SHOTS = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEg-rIjUDdgg"
      },
      "outputs": [],
      "source": [
        "class SoftDiceLoss(nn.Module):\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "    def forward(self, logits, targets):\n",
        "        probs = torch.sigmoid(logits)\n",
        "        num = 2 * (probs * targets).sum(dim=(1,2,3)) + self.eps\n",
        "        den = probs.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3)) + self.eps\n",
        "        return (1 - num / den).mean()\n",
        "\n",
        "class FocalLossBinary(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.alpha, self.gamma, self.reduction = alpha, gamma, reduction\n",
        "    def forward(self, logits, targets):\n",
        "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
        "        p = torch.sigmoid(logits)\n",
        "        pt = targets * p + (1 - targets) * (1 - p)\n",
        "        loss = (self.alpha * (1 - pt) ** self.gamma) * bce\n",
        "        return loss.mean() if self.reduction==\"mean\" else (loss.sum() if self.reduction==\"sum\" else loss)\n",
        "\n",
        "soft_dice_loss = SoftDiceLoss()\n",
        "focal_loss = FocalLossBinary()\n",
        "\n",
        "def improved_seg_loss(logits, gt):\n",
        "    return (0.6 * soft_dice_loss(logits, gt)\n",
        "          + 0.2 * F.binary_cross_entropy_with_logits(logits, gt)\n",
        "          + 0.2 * focal_loss(logits, gt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Sx2JX0iDLNa"
      },
      "outputs": [],
      "source": [
        "def load_image_for_clip(path, processor):\n",
        "    pil = Image.open(path).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n",
        "    orig_size = pil.size[::-1]\n",
        "    pix = processor(images=pil, return_tensors=\"pt\")[\"pixel_values\"].to(DEVICE)\n",
        "    return pix, orig_size, pil\n",
        "\n",
        "def load_mask_orig(path):\n",
        "    m = Image.open(path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE), resample=Image.NEAREST)\n",
        "    arr = (np.array(m) > 127).astype(np.float32)\n",
        "    return torch.from_numpy(arr).to(DEVICE)\n",
        "\n",
        "def load_image_for_clip_from_pil(pil, processor):\n",
        "    return processor(images=pil, return_tensors=\"pt\")[\"pixel_values\"].to(DEVICE)\n",
        "\n",
        "def pil_mask_to_tensor01(pil_mask):\n",
        "    arr = (np.array(pil_mask.convert(\"L\")) > 127).astype(np.float32)\n",
        "    return torch.from_numpy(arr).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8l1Ab_Au-uY"
      },
      "outputs": [],
      "source": [
        "class PairedImageDataset:\n",
        "    def __init__(self, image_dir, mask_dir):\n",
        "        img_files = sorted([p for p in Path(image_dir).glob(\"*\") if p.suffix.lower() in [\".jpg\",\".png\"]])\n",
        "        mask_files = sorted([p for p in Path(mask_dir).glob(\"*\") if p.suffix.lower() in [\".jpg\",\".png\"]])\n",
        "        mask_map = {m.name: m for m in mask_files}\n",
        "        self.pairs = [(i, mask_map.get(i.name)) for i in img_files if i.name in mask_map]\n",
        "        self.pairs = [(i,m) for i,m in self.pairs if m is not None]\n",
        "        print(\"Total matched pairs:\", len(self.pairs))\n",
        "    def sample_episode(self, k=3):\n",
        "        idxs = random.sample(range(len(self.pairs)), k+1)\n",
        "        q_idx, sup_idxs = idxs[0], idxs[1:]\n",
        "        q_img_path, q_mask_path = self.pairs[q_idx]\n",
        "        supports = [self.pairs[i] for i in sup_idxs]\n",
        "        return q_img_path, q_mask_path, supports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y1IyNUyvL-R"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(p_drop)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class UNetProto(nn.Module):\n",
        "    def __init__(self, in_ch=3, base_ch=32):\n",
        "        super().__init__()\n",
        "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.bottleneck = ConvBlock(base_ch*4, base_ch*8)\n",
        "        self.up2 = nn.ConvTranspose2d(base_ch*8, base_ch*4, 2, stride=2)\n",
        "        self.dec2 = ConvBlock(base_ch*4, base_ch*4)\n",
        "        self.up1 = nn.ConvTranspose2d(base_ch*4, base_ch*2, 2, stride=2)\n",
        "        self.dec1 = ConvBlock(base_ch*2, base_ch*2)\n",
        "        self.final_up = nn.ConvTranspose2d(base_ch*2, base_ch, 2, stride=2)\n",
        "        self.final = nn.Conv2d(base_ch, 1, 1)\n",
        "\n",
        "    def forward_encoder(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool1(e1))\n",
        "        e3 = self.enc3(self.pool2(e2))\n",
        "        b = self.bottleneck(self.pool3(e3))\n",
        "        return {\n",
        "            'bottleneck': b,\n",
        "            'e3': e3,\n",
        "            'e2': e2,\n",
        "            'e1': e1,\n",
        "        }\n",
        "\n",
        "    def forward_decoder(self, feat):\n",
        "        x = self.up2(feat)\n",
        "        x = self.dec2(x)\n",
        "        x = self.up1(x)\n",
        "        x = self.dec1(x)\n",
        "        x = self.final_up(x)\n",
        "        out = self.final(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFewUf5uvUIZ"
      },
      "outputs": [],
      "source": [
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, clip_model, classnames, ctx_len=8):\n",
        "        super().__init__()\n",
        "        self.clip = clip_model\n",
        "        self.classnames = classnames\n",
        "        self.ctx_len = ctx_len\n",
        "        self.ctx = nn.Parameter(torch.randn(ctx_len, clip_model.text_model.config.hidden_size))\n",
        "\n",
        "        self.text_proj = nn.Linear(\n",
        "            clip_model.text_model.config.hidden_size,\n",
        "            clip_model.vision_model.config.hidden_size\n",
        "        )\n",
        "\n",
        "    def forward(self, processor):\n",
        "        tokens = [f\"a photo of a {c}\" for c in self.classnames]\n",
        "        inputs = processor(text=tokens, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "        outputs = self.clip.text_model(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "        txt_embeds = outputs.hidden_states[-1]\n",
        "\n",
        "        ctx_exp = self.ctx.unsqueeze(0).expand(txt_embeds.size(0), -1, -1)\n",
        "        txt_full = torch.cat([ctx_exp, txt_embeds], dim=1)\n",
        "        txt_proj = self.text_proj(txt_full)\n",
        "        return txt_proj\n",
        "\n",
        "class PrototypeRefiner(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(dim, dim)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, patch_tokens, mask_small):\n",
        "        msum = mask_small.sum().clamp(min=1e-6)\n",
        "        proto = (patch_tokens * mask_small.unsqueeze(-1)).sum(dim=0) / msum\n",
        "        q = self.lin(proto).unsqueeze(0)\n",
        "        logits = (patch_tokens @ q.t()).squeeze(-1)\n",
        "        w = torch.softmax(logits * mask_small, dim=0)\n",
        "        refined = (w.unsqueeze(-1) * patch_tokens).sum(dim=0)\n",
        "        proto = self.norm(proto + refined)\n",
        "        return proto\n",
        "\n",
        "class CrossAttentionFusion(nn.Module):\n",
        "    def __init__(self, dim, n_heads=8):\n",
        "        super().__init__()\n",
        "        self.mha = nn.MultiheadAttention(dim, n_heads, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.ff = nn.Sequential(nn.Linear(dim, dim*4), nn.GELU(), nn.Linear(dim*4, dim))\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, query_tokens, support_protos):\n",
        "        B, N, D = query_tokens.shape\n",
        "        if support_protos.dim() == 2:\n",
        "            key = support_protos.unsqueeze(0).repeat(B, 1, 1)\n",
        "        else:\n",
        "            key = support_protos\n",
        "\n",
        "        attn_out, _ = self.mha(query_tokens, key, key)\n",
        "        x = self.norm1(query_tokens + attn_out)\n",
        "        x = self.norm2(x + self.ff(x))\n",
        "        return x\n",
        "\n",
        "class UNetDecoderFromPatches(nn.Module):\n",
        "    def __init__(self, in_dim, mid_ch=256):\n",
        "        super().__init__()\n",
        "        self.proj1 = nn.Linear(in_dim, mid_ch)\n",
        "        self.proj2 = nn.Linear(in_dim, mid_ch)\n",
        "        self.proj3 = nn.Linear(in_dim, mid_ch)\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(mid_ch, mid_ch, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(mid_ch, mid_ch, 3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(mid_ch*2, mid_ch, 3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(mid_ch*2, mid_ch, 3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.final = nn.Conv2d(mid_ch, 1, 1)\n",
        "\n",
        "    def forward(self, feats, out_size):\n",
        "        B, N, D = feats[0].shape\n",
        "        sz = int(N ** 0.5)\n",
        "\n",
        "        f1 = self.proj1(feats[0])\n",
        "        f1 = rearrange(f1, 'b (h w) c -> b c h w', h=sz)\n",
        "        f1 = F.interpolate(f1, scale_factor=4, mode='bilinear', align_corners=False)\n",
        "\n",
        "        f2 = self.proj2(feats[1])\n",
        "        f2 = rearrange(f2, 'b (h w) c -> b c h w', h=sz)\n",
        "        f2 = F.interpolate(f2, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        f3 = self.proj3(feats[2])\n",
        "        f3 = rearrange(f3, 'b (h w) c -> b c h w', h=sz)\n",
        "\n",
        "        p1 = self.conv1(f3)\n",
        "        p1_up = F.interpolate(p1, size=f2.shape[2:], mode='bilinear', align_corners=False)\n",
        "        p2 = self.conv2(torch.cat([p1_up, f2], dim=1))\n",
        "\n",
        "        p2_up = F.interpolate(p2, size=f1.shape[2:], mode='bilinear', align_corners=False)\n",
        "        p3 = self.conv3(torch.cat([p2_up, f1], dim=1))\n",
        "\n",
        "        out = self.final(p3)\n",
        "        out = F.interpolate(out, size=out_size, mode='bilinear', align_corners=False)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NcJym5xvav_"
      },
      "outputs": [],
      "source": [
        "class IntegratedFewShotModel(nn.Module):\n",
        "    def __init__(self, clip_model, processor, classnames=[\"polyp\"], base_ch=32):\n",
        "        super().__init__()\n",
        "        self.unet = UNetProto(in_ch=3, base_ch=base_ch).to(DEVICE)\n",
        "\n",
        "        self.clip = clip_model.to(DEVICE)\n",
        "        self.processor = processor\n",
        "\n",
        "        self.prompt_learner = PromptLearner(self.clip, classnames).to(DEVICE)\n",
        "\n",
        "        self.refiner = PrototypeRefiner(dim=self.clip.vision_model.config.hidden_size).to(DEVICE)\n",
        "        self.fusion = CrossAttentionFusion(dim=self.clip.vision_model.config.hidden_size, n_heads=8).to(DEVICE)\n",
        "        self.clip_decoder = UNetDecoderFromPatches(in_dim=self.clip.vision_model.config.hidden_size, mid_ch=256).to(DEVICE)\n",
        "\n",
        "        self.text_proj = nn.Linear(self.clip.text_model.config.hidden_size,\n",
        "                           self.clip.vision_model.config.hidden_size).to(DEVICE)\n",
        "\n",
        "        self.unet_to_clip = nn.Linear(8*base_ch, self.clip.vision_model.config.hidden_size)\n",
        "\n",
        "        for p in self.clip.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def compute_clilp_patch_tokens(self, pixel_values):\n",
        "        outputs = self.clip.vision_model(pixel_values=pixel_values, output_hidden_states=True)\n",
        "        hidden = outputs.hidden_states\n",
        "        idxs = [len(hidden)//3, 2*len(hidden)//3, len(hidden)-1]\n",
        "        feats = []\n",
        "        for i in idxs:\n",
        "            h = hidden[i]\n",
        "            h = h[:, 1:, :].contiguous()\n",
        "            feats.append(h)\n",
        "        return feats\n",
        "\n",
        "    def compute_support_proto(self, sup_pixel_values, sup_mask_tensor):\n",
        "        feats = self.compute_clilp_patch_tokens(sup_pixel_values)\n",
        "        finest = feats[-1].squeeze(0)\n",
        "        N = finest.shape[0]\n",
        "        sz = int(N ** 0.5)\n",
        "\n",
        "        mask_small = F.interpolate(sup_mask_tensor.unsqueeze(0).unsqueeze(0),\n",
        "                                   size=(sz, sz), mode='nearest').view(-1)\n",
        "        mask_small = mask_small.to(finest.dtype)\n",
        "\n",
        "        proto = self.refiner(finest, mask_small)\n",
        "        return proto, feats\n",
        "\n",
        "    def forward_episode(self, q_img_path, q_mask_path, supports):\n",
        "        q_pix, q_size, q_pil = load_image_for_clip(q_img_path, self.processor)\n",
        "        q_mask_orig = load_mask_orig(q_mask_path)\n",
        "        q_feats = self.compute_clilp_patch_tokens(q_pix)\n",
        "        q_patch_tokens = q_feats[-1]\n",
        "\n",
        "        protos = []\n",
        "        for s_img_path, s_mask_path in supports:\n",
        "            s_pix, s_size, _ = load_image_for_clip(s_img_path, self.processor)\n",
        "            s_mask_orig = load_mask_orig(s_mask_path)\n",
        "            proto, _ = self.compute_support_proto(s_pix, s_mask_orig)\n",
        "            protos.append(proto)\n",
        "        protos = torch.stack(protos, dim=0)\n",
        "        proto_mean = protos.mean(dim=0)\n",
        "        proto_tokens = proto_mean.unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "        text_tokens = self.prompt_learner(self.processor)\n",
        "        text_tokens = text_tokens.mean(dim=0, keepdim=True)\n",
        "\n",
        "        all_proto = torch.cat([proto_tokens, text_tokens], dim=1)\n",
        "        fused = self.fusion(q_patch_tokens, all_proto)\n",
        "\n",
        "        decoder_feats = [q_feats[0], q_feats[1], fused]\n",
        "        mask_logits_clip = self.clip_decoder(decoder_feats, out_size=q_mask_orig.shape)\n",
        "\n",
        "        q_pil_unet = Image.open(q_img_path).convert('RGB').resize((IMG_SIZE, IMG_SIZE))\n",
        "        q_tensor_unet = torch.from_numpy(np.array(q_pil_unet).transpose(2,0,1)).float().unsqueeze(0) / 255.0\n",
        "        q_tensor_unet = q_tensor_unet.to(DEVICE)\n",
        "\n",
        "        unet_feats = self.unet.forward_encoder(q_tensor_unet)\n",
        "        unet_bottleneck = unet_feats['bottleneck']\n",
        "\n",
        "        B, C, Hb, Wb = unet_bottleneck.shape\n",
        "        unet_vec = unet_bottleneck.view(B, C, -1).mean(dim=-1)\n",
        "        clip_aligned = self.unet_to_clip(unet_vec)\n",
        "\n",
        "        fused_proto = fused.mean(dim=1)\n",
        "        proto = fused_proto.view(1, -1)\n",
        "\n",
        "        sim = F.cosine_similarity(clip_aligned.unsqueeze(-1), proto.unsqueeze(-1), dim=1)\n",
        "        sim_map = sim.view(B,1,1,1).expand(B,1,Hb,Wb)\n",
        "\n",
        "        fused_bottleneck = unet_bottleneck * sim_map\n",
        "        mask_logits_unet = self.unet.forward_decoder(fused_bottleneck)\n",
        "        mask_logits_unet = F.interpolate(mask_logits_unet, size=q_mask_orig.shape, mode='bilinear', align_corners=False)\n",
        "\n",
        "        if mask_logits_clip.shape != mask_logits_unet.shape:\n",
        "            mask_logits_unet = F.interpolate(mask_logits_unet, size=mask_logits_clip.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        mask_logits = 0.5 * mask_logits_clip + 0.5 * mask_logits_unet\n",
        "\n",
        "        return mask_logits, q_mask_orig, q_pil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-S4umtpNvzeS"
      },
      "outputs": [],
      "source": [
        "def combined_bce_dice_loss(logits, gt):\n",
        "    loss_bce = F.binary_cross_entropy_with_logits(logits, gt)\n",
        "    pred = torch.sigmoid(logits)\n",
        "    intersection = (pred * gt).sum()\n",
        "    loss_dice = 1 - (2 * intersection + 1) / (pred.sum() + gt.sum() + 1)\n",
        "    return loss_bce + loss_dice\n",
        "\n",
        "def build_and_train(num_epochs=10, episodes_per_epoch=30, lr=3e-4, wd=1e-4):\n",
        "    clip_name = \"openai/clip-vit-base-patch32\"\n",
        "    clip_model = CLIPModel.from_pretrained(clip_name).to(DEVICE)\n",
        "    processor = CLIPProcessor.from_pretrained(clip_name)\n",
        "    print('Loaded CLIP', clip_name)\n",
        "\n",
        "    integrated = IntegratedFewShotModel(clip_model, processor).to(DEVICE)\n",
        "\n",
        "    trainable = list(integrated.prompt_learner.parameters()) + \\\n",
        "                list(integrated.refiner.parameters()) + \\\n",
        "                list(integrated.fusion.parameters()) + \\\n",
        "                list(integrated.clip_decoder.parameters()) + \\\n",
        "                list(integrated.unet_to_clip.parameters()) + \\\n",
        "                list(integrated.unet.parameters())\n",
        "\n",
        "    optimizer = optim.AdamW(trainable, lr=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE=='cuda'))\n",
        "\n",
        "    dataset = PairedImageDataset('polyp_data/images', 'polyp_data/masks')\n",
        "\n",
        "    for epoch in trange(num_epochs, desc=\"Training Epochs\"):\n",
        "        integrated.train()\n",
        "        total_loss = 0.0\n",
        "        for _ in trange(episodes_per_epoch, leave=False, desc=f\"Epoch {epoch+1}\"):\n",
        "            q_img_path, q_mask_path, supports = dataset.sample_episode(k=SUPPORT_SHOTS)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=(DEVICE=='cuda')):\n",
        "                mask_logits, gt_mask, _ = integrated.forward_episode(\n",
        "                    q_img_path, q_mask_path, supports\n",
        "                )\n",
        "                gt = gt_mask.unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
        "                loss = improved_seg_loss(mask_logits, gt)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(trainable, max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        scheduler.step(epoch)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Avg Loss: {total_loss/episodes_per_epoch:.4f}\")\n",
        "\n",
        "    return integrated, dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3JNOFcyv7eo"
      },
      "outputs": [],
      "source": [
        "def dice_score(pred, target, eps=1e-6):\n",
        "    pred = pred.float()\n",
        "    target = target.float()\n",
        "    inter = (pred * target).sum()\n",
        "    return (2 * inter + eps) / (pred.sum() + target.sum() + eps)\n",
        "\n",
        "def iou_score(pred, target, eps=1e-6):\n",
        "    pred = pred.float()\n",
        "    target = target.float()\n",
        "    inter = (pred * target).sum()\n",
        "    union = pred.sum() + target.sum() - inter\n",
        "    return (inter + eps) / (union + eps)\n",
        "\n",
        "def visualize_inference(model_inst, dataset_inst, n=3, thr=0.5):\n",
        "    model_inst.eval()\n",
        "    dice_scores, iou_scores = [], []\n",
        "\n",
        "    plt.figure(figsize=(12, 4 * n))\n",
        "    for i in range(n):\n",
        "        q_img_path, q_mask_path, supports = dataset_inst.sample_episode(k=SUPPORT_SHOTS)\n",
        "\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=(DEVICE=='cuda')):\n",
        "            logits, gt_mask, q_pil = model_inst.forward_episode(\n",
        "                q_img_path, q_mask_path, supports\n",
        "            )\n",
        "            probs = torch.sigmoid(logits).squeeze().float().cpu()\n",
        "            pp = postprocess_prob(probs, thr=thr, min_size=64)\n",
        "            pred = pp.float()\n",
        "\n",
        "        gt = gt_mask.unsqueeze(0).unsqueeze(0).cpu()\n",
        "        dice = dice_score(pred, gt)\n",
        "        iou = iou_score(pred, gt)\n",
        "        dice_scores.append(dice.item()); iou_scores.append(iou.item())\n",
        "\n",
        "        plt.subplot(n, 3, i * 3 + 1); plt.imshow(q_pil); plt.title(\"Query\"); plt.axis(\"off\")\n",
        "        plt.subplot(n, 3, i * 3 + 2); plt.imshow(gt.squeeze(), cmap=\"gray\"); plt.title(\"GT\"); plt.axis(\"off\")\n",
        "        plt.subplot(n, 3, i * 3 + 3); plt.imshow(pred.squeeze(), cmap=\"gray\")\n",
        "        plt.title(f\"Pred\\nDice:{dice:.3f}, IoU:{iou:.3f}\"); plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout(); plt.show()\n",
        "    print(f\"Mean Dice: {np.mean(dice_scores):.4f} | Mean IoU: {np.mean(iou_scores):.4f}\")\n",
        "    return dice_scores, iou_scores\n",
        "\n",
        "def evaluate_model(model_inst, dataset_inst, num_samples=50, thr=0.5):\n",
        "    model_inst.eval()\n",
        "    dice_scores, iou_scores = [], []\n",
        "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=(DEVICE=='cuda')):\n",
        "        for _ in range(num_samples):\n",
        "            q_img_path, q_mask_path, supports = dataset_inst.sample_episode(k=SUPPORT_SHOTS)\n",
        "            logits, gt_mask, _ = model_inst.forward_episode(\n",
        "                q_img_path, q_mask_path, supports\n",
        "            )\n",
        "            probs = torch.sigmoid(logits).squeeze().float().cpu()\n",
        "            pred = postprocess_prob(probs, thr=thr, min_size=64).float()\n",
        "            gt = gt_mask.unsqueeze(0).unsqueeze(0).cpu()\n",
        "\n",
        "            dice = dice_score(pred, gt); iou = iou_score(pred, gt)\n",
        "            dice_scores.append(dice.item()); iou_scores.append(iou.item())\n",
        "\n",
        "    print(f\"Evaluation on {num_samples} episodes:\")\n",
        "    print(f\"Mean Dice: {np.mean(dice_scores):.4f} ± {np.std(dice_scores):.4f}\")\n",
        "    print(f\"Mean IoU: {np.mean(iou_scores):.4f} ± {np.std(iou_scores):.4f}\")\n",
        "    return dice_scores, iou_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N5-3cMLLDiN"
      },
      "outputs": [],
      "source": [
        "def find_global_threshold(model_inst, dataset_inst, episodes=30, thr_grid=None):\n",
        "    if thr_grid is None:\n",
        "        thr_grid = np.linspace(0.35, 0.65, 7)\n",
        "    best_thr, best_dice = 0.5, -1\n",
        "    for thr in thr_grid:\n",
        "        ds, _ = evaluate_model(model_inst, dataset_inst, num_samples=episodes, thr=thr)\n",
        "        m = np.mean(ds)\n",
        "        print(f\"thr={thr:.2f} -> mean Dice {m:.4f}\")\n",
        "        if m > best_dice:\n",
        "            best_dice, best_thr = m, thr\n",
        "    print(f\"[Threshold search] Best thr={best_thr:.2f} with mean Dice={best_dice:.4f}\")\n",
        "    return best_thr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldIoKeexDyM7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from skimage.morphology import remove_small_holes, remove_small_objects, closing, disk\n",
        "from skimage.measure import label\n",
        "\n",
        "def postprocess_prob(prob_2d, thr=0.5, min_size=64, pad=False):\n",
        "    if torch.is_tensor(prob_2d):\n",
        "        prob = prob_2d.detach().cpu().numpy()\n",
        "    else:\n",
        "        prob = prob_2d\n",
        "    mask = (prob >= thr).astype(np.uint8)\n",
        "    lab = label(mask)\n",
        "    mask = lab > 0\n",
        "    mask = remove_small_objects(mask, min_size=min_size)\n",
        "    mask = remove_small_holes(mask, area_threshold=min_size)\n",
        "    mask = closing(mask, disk(3))\n",
        "    mask = torch.from_numpy(mask.astype(np.float32)).unsqueeze(0).unsqueeze(0)\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iPVVf1qzyoNJ",
        "outputId": "d0714447-509a-4d06-fdaf-bb159f8550ce"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    trained_model, dataset = build_and_train(num_epochs=10, episodes_per_epoch=60)\n",
        "\n",
        "    print(\"\\nSearching best inference threshold (optional):\")\n",
        "    best_thr = find_global_threshold(trained_model, dataset, episodes=15)\n",
        "\n",
        "    print(\"\\nVisualizing some inference results:\")\n",
        "    visualize_inference(trained_model, dataset, n=3, thr=best_thr)\n",
        "\n",
        "    print(\"\\nEvaluating model performance:\")\n",
        "    evaluate_model(trained_model, dataset, num_samples=20, thr=best_thr)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
